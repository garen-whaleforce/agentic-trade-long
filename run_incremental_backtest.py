#!/usr/bin/env python3
"""
å¢é‡å›æ¸¬è…³æœ¬ - æ”¯æŒ Checkpoint æ©Ÿåˆ¶
Incremental Backtest with Checkpoint Support

ä½¿ç”¨æ–¹å¼:
1. å…ˆæ¸¬è©¦ 10 ç­†: python3 run_incremental_backtest.py --test 10
2. å®Œæ•´å›æ¸¬: python3 run_incremental_backtest.py --full
3. å¾ checkpoint ç¹¼çºŒ: python3 run_incremental_backtest.py --resume
"""

import sys
import time
import json
import argparse
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import pandas as pd
from dotenv import load_dotenv

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

load_dotenv()

from agentic_rag_bridge import run_single_call_from_context

# Checkpoint è¨­å®š
CHECKPOINT_DIR = Path("backtest_checkpoints")
CHECKPOINT_FILE = CHECKPOINT_DIR / "checkpoint.json"
RESULTS_FILE = CHECKPOINT_DIR / "backtest_results.json"
CHECKPOINT_INTERVAL = 100  # æ¯ 100 ç­†å„²å­˜ä¸€æ¬¡
DEFAULT_WORKERS = 5  # é è¨­ä¸¦ç™¼æ•¸é‡

# ç¢ºä¿ checkpoint ç›®éŒ„å­˜åœ¨
CHECKPOINT_DIR.mkdir(exist_ok=True)

# ä¸¦ç™¼æ§åˆ¶
results_lock = threading.Lock()
checkpoint_lock = threading.Lock()


def load_earnings_calendar() -> pd.DataFrame:
    """è¼‰å…¥è²¡å ±æ—¥æ›†è³‡æ–™ã€‚

    å¾ earnings_transcripts è¡¨è¼‰å…¥ 2017-2024 çš„è²¡å ±æ—¥æ›†ã€‚
    """
    try:
        import pg_client

        # ä½¿ç”¨ get_cursor() context manager
        with pg_client.get_cursor() as cur:
            if cur is None:
                print("âš ï¸  è³‡æ–™åº«é€£æ¥å¤±æ•—ï¼Œä½¿ç”¨æ¸¬è©¦è³‡æ–™")
                return create_test_data()

            query = """
                SELECT DISTINCT
                    et.symbol,
                    et.year,
                    et.quarter,
                    et.transcript_date,
                    c.sector
                FROM earnings_transcripts et
                LEFT JOIN companies c ON et.symbol = c.symbol
                WHERE et.year BETWEEN 2017 AND 2024
                  AND et.transcript_date IS NOT NULL
                ORDER BY et.transcript_date, et.symbol
            """

            cur.execute(query)
            result = cur.fetchall()

            if result:
                df = pd.DataFrame(result)
                print(f"âœ… è¼‰å…¥ {len(df)} ç­†è²¡å ±è¨˜éŒ„ (2017-2024)")
                return df
            else:
                print("âš ï¸  æœªæ‰¾åˆ°è²¡å ±è¨˜éŒ„ï¼Œä½¿ç”¨æ¸¬è©¦è³‡æ–™")
                return create_test_data()

    except Exception as e:
        print(f"âš ï¸  è¼‰å…¥è²¡å ±æ—¥æ›†å¤±æ•—: {e}")
        print("ä½¿ç”¨æ¸¬è©¦è³‡æ–™")
        return create_test_data()


def create_test_data() -> pd.DataFrame:
    """å‰µå»ºæ¸¬è©¦è³‡æ–™ (å¦‚æœæ²’æœ‰å¯¦éš›è³‡æ–™)ã€‚"""
    test_data = []

    # 2024 Q1 æ¸¬è©¦è³‡æ–™
    symbols = ["AAPL", "MSFT", "GOOGL", "NVDA", "META", "AMZN", "TSLA",
               "JPM", "JNJ", "XOM", "AMD", "NFLX", "NKE", "HD", "BA"]

    for symbol in symbols:
        test_data.append({
            "symbol": symbol,
            "year": 2024,
            "quarter": 1,
            "transcript_date": "2024-02-01",
            "sector": "Technology"
        })

    return pd.DataFrame(test_data)


def load_checkpoint() -> Dict[str, Any]:
    """è¼‰å…¥ checkpointã€‚"""
    if CHECKPOINT_FILE.exists():
        with open(CHECKPOINT_FILE, "r") as f:
            checkpoint = json.load(f)
        print(f"âœ… è¼‰å…¥ checkpoint: å·²è™•ç† {checkpoint['processed_count']} ç­†")
        return checkpoint
    return {
        "processed_count": 0,
        "processed_ids": [],
        "last_update": None
    }


def save_checkpoint(checkpoint: Dict[str, Any]):
    """å„²å­˜ checkpointã€‚"""
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    CHECKPOINT_DIR.mkdir(exist_ok=True)
    checkpoint["last_update"] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, "w") as f:
        json.dump(checkpoint, f, indent=2)
    print(f"ğŸ’¾ Checkpoint å·²å„²å­˜: {checkpoint['processed_count']} ç­†")


def load_results() -> List[Dict[str, Any]]:
    """è¼‰å…¥å·²æœ‰çš„çµæœã€‚"""
    if RESULTS_FILE.exists():
        with open(RESULTS_FILE, "r") as f:
            data = json.load(f)
            return data.get("results", [])
    return []


def save_results(results: List[Dict[str, Any]], checkpoint: Dict[str, Any],
                 stats: Dict[str, Any]):
    """å„²å­˜çµæœã€‚"""
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    CHECKPOINT_DIR.mkdir(exist_ok=True)
    output = {
        "last_update": datetime.now().isoformat(),
        "total_processed": checkpoint["processed_count"],
        "statistics": stats,
        "results": results
    }

    with open(RESULTS_FILE, "w") as f:
        json.dump(output, f, indent=2)

    print(f"ğŸ’¾ çµæœå·²å„²å­˜: {len(results)} ç­†è¨˜éŒ„")


def calculate_statistics(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¨ˆç®—çµ±è¨ˆè³‡è¨Šã€‚"""
    if not results:
        return {}

    df = pd.DataFrame(results)

    # éæ¿¾æˆåŠŸçš„çµæœ
    if "error" in df.columns:
        successful = df[df["error"].isna()]
    else:
        successful = df

    if len(successful) == 0:
        return {"error": "No successful results"}

    # Direction Score åˆ†ä½ˆ
    direction_dist = successful["direction_score"].value_counts().to_dict()

    # Tier åˆ†ä½ˆ
    trade_long_df = successful[successful["trade_long"]]
    tier_dist = {}
    if len(trade_long_df) > 0:
        tier_dist = trade_long_df["tier"].value_counts().to_dict()

    # äº¤æ˜“ä¿¡è™Ÿçµ±è¨ˆ
    trade_count = successful["trade_long"].sum()
    trade_rate = trade_count / len(successful) * 100 if len(successful) > 0 else 0

    # é«˜éš Tier çµ±è¨ˆ
    high_tier_count = sum(1 for t in trade_long_df["tier"] if t in ["D8_MEGA", "D7_CORE", "D6_STRICT"])
    high_tier_ratio = high_tier_count / len(trade_long_df) * 100 if len(trade_long_df) > 0 else 0

    # æ™‚é–“çµ±è¨ˆ
    avg_time = successful["time_seconds"].mean()
    total_time = successful["time_seconds"].sum()

    # EPS Surprise çµ±è¨ˆ
    eps_available = successful[successful["eps_surprise"].notna()]
    eps_stats = {}
    if len(eps_available) > 0:
        eps_stats = {
            "mean": float(eps_available["eps_surprise"].mean()),
            "median": float(eps_available["eps_surprise"].median()),
            "min": float(eps_available["eps_surprise"].min()),
            "max": float(eps_available["eps_surprise"].max()),
        }

    return {
        "total_analyzed": len(successful),
        "total_errors": len(df) - len(successful),
        "direction_distribution": direction_dist,
        "tier_distribution": tier_dist,
        "trade_signals": {
            "count": int(trade_count),
            "rate": float(trade_rate)
        },
        "high_tier_stats": {
            "count": high_tier_count,
            "ratio": float(high_tier_ratio)
        },
        "performance": {
            "avg_time_seconds": float(avg_time),
            "total_time_seconds": float(total_time),
            "total_time_hours": float(total_time / 3600)
        },
        "eps_surprise": eps_stats
    }


def print_statistics(stats: Dict[str, Any], processed_count: int):
    """æ‰“å°çµ±è¨ˆè³‡è¨Šã€‚"""
    print("\n" + "=" * 80)
    print(f"ğŸ“Š ç•¶å‰çµ±è¨ˆ (å·²è™•ç† {processed_count} ç­†)")
    print("=" * 80)

    if not stats or "error" in stats:
        print("âš ï¸  æš«ç„¡çµ±è¨ˆè³‡è¨Š")
        return

    print(f"\nâœ… æˆåŠŸåˆ†æ: {stats['total_analyzed']} ç­†")
    if stats['total_errors'] > 0:
        print(f"âŒ éŒ¯èª¤: {stats['total_errors']} ç­†")

    # Direction Score
    print(f"\nğŸ“Š Direction Score åˆ†ä½ˆ:")
    direction_dist = stats.get("direction_distribution", {})
    for score in sorted(direction_dist.keys(), reverse=True):
        count = direction_dist[score]
        pct = count / stats['total_analyzed'] * 100
        bar = "â–ˆ" * max(1, int(pct / 5))
        print(f"  D{score}: {count:4d} ({pct:5.1f}%) {bar}")

    # Tier åˆ†ä½ˆ
    tier_dist = stats.get("tier_distribution", {})
    if tier_dist:
        print(f"\nğŸ† Tier åˆ†ä½ˆ:")
        for tier in ["D8_MEGA", "D7_CORE", "D6_STRICT", "D5_GATED", "D4_ENTRY", "D4_OPP", "D3_WIDE"]:
            if tier in tier_dist:
                count = tier_dist[tier]
                print(f"  {tier}: {count}")

    # äº¤æ˜“ä¿¡è™Ÿ
    trade_stats = stats.get("trade_signals", {})
    print(f"\nâœ… äº¤æ˜“ä¿¡è™Ÿ: {trade_stats.get('count', 0)} ({trade_stats.get('rate', 0):.1f}%)")

    # é«˜éš Tier
    high_tier = stats.get("high_tier_stats", {})
    print(f"   é«˜éš Tier (D6+): {high_tier.get('count', 0)} ({high_tier.get('ratio', 0):.1f}%)")

    # æ€§èƒ½
    perf = stats.get("performance", {})
    print(f"\nâ±ï¸  æ€§èƒ½çµ±è¨ˆ:")
    print(f"   å¹³å‡æ™‚é–“: {perf.get('avg_time_seconds', 0):.1f}s per call")
    print(f"   ç¸½æ™‚é–“: {perf.get('total_time_hours', 0):.2f} hours")

    # EPS Surprise
    eps_stats = stats.get("eps_surprise", {})
    if eps_stats:
        print(f"\nğŸ’° EPS Surprise çµ±è¨ˆ:")
        print(f"   å¹³å‡: {eps_stats.get('mean', 0):.1%}")
        print(f"   ç¯„åœ: {eps_stats.get('min', 0):.1%} ~ {eps_stats.get('max', 0):.1%}")

    print("=" * 80 + "\n")


def process_single_case(row_data: Dict[str, Any]) -> Dict[str, Any]:
    """è™•ç†å–®ç­†æ¡ˆä¾‹çš„ worker å‡½æ•¸ã€‚

    Args:
        row_data: åŒ…å« symbol, year, quarter, record_id çš„å­—å…¸

    Returns:
        è™•ç†çµæœå­—å…¸
    """
    symbol = row_data["symbol"]
    year = row_data["year"]
    quarter = row_data["quarter"]
    record_id = row_data["record_id"]

    try:
        case_start = time.time()

        context = {
            "symbol": symbol,
            "year": year,
            "quarter": quarter,
            "transcript_date": None,
        }

        result = run_single_call_from_context(context)
        case_time = time.time() - case_start

        # æå–çµæœ
        long_eligible = result.get("long_eligible_json", {})
        direction_score = long_eligible.get("DirectionScore", 0)
        confidence = result.get("confidence", 0)
        trade_long = result.get("trade_long", False)
        tier = result.get("trade_long_tier", "")

        market_anchors = result.get("market_anchors", {})
        eps_surprise = market_anchors.get("eps_surprise")

        return {
            "id": record_id,
            "symbol": symbol,
            "year": year,
            "quarter": quarter,
            "direction_score": direction_score,
            "confidence": confidence,
            "trade_long": trade_long,
            "tier": tier,
            "eps_surprise": eps_surprise,
            "time_seconds": case_time,
            "success": True
        }

    except Exception as e:
        error_msg = str(e)[:100]
        return {
            "id": record_id,
            "symbol": symbol,
            "year": year,
            "quarter": quarter,
            "error": error_msg,
            "time_seconds": 0,
            "success": False
        }


def run_backtest(calendar_df: pd.DataFrame, max_samples: Optional[int] = None,
                 resume: bool = False, workers: int = DEFAULT_WORKERS):
    """åŸ·è¡Œå›æ¸¬ã€‚

    Args:
        calendar_df: è²¡å ±æ—¥æ›† DataFrame
        max_samples: æœ€å¤§æ¨£æœ¬æ•¸ (None = å…¨éƒ¨)
        resume: æ˜¯å¦å¾ checkpoint ç¹¼çºŒ
        workers: ä¸¦ç™¼æ•¸é‡ï¼ˆ1 = ä¸²è¡Œï¼Œ>1 = ä¸¦ç™¼ï¼‰
    """
    # è¼‰å…¥ checkpoint å’Œçµæœ
    checkpoint = load_checkpoint() if resume else {
        "processed_count": 0,
        "processed_ids": [],
        "last_update": None
    }

    results = load_results() if resume else []

    # éæ¿¾å·²è™•ç†çš„è¨˜éŒ„
    processed_ids = set(checkpoint["processed_ids"])
    calendar_df["id"] = calendar_df.apply(
        lambda row: f"{row['symbol']}_{row['year']}Q{row['quarter']}",
        axis=1
    )

    if resume:
        calendar_df = calendar_df[~calendar_df["id"].isin(processed_ids)]
        print(f"ğŸ”„ å¾ checkpoint ç¹¼çºŒ: å‰©é¤˜ {len(calendar_df)} ç­†")

    # é™åˆ¶æ¨£æœ¬æ•¸
    if max_samples:
        calendar_df = calendar_df.head(max_samples)

    total_samples = len(calendar_df)
    if total_samples == 0:
        print("âœ… æ‰€æœ‰è¨˜éŒ„å·²è™•ç†å®Œæˆ")
        return

    print("\n" + "=" * 80)
    print(f"ğŸš€ é–‹å§‹å›æ¸¬ ({'ä¸¦ç™¼' if workers > 1 else 'ä¸²è¡Œ'}æ¨¡å¼)")
    print("=" * 80)
    print(f"ç¸½æ¨£æœ¬æ•¸: {total_samples}")
    print(f"å·²è™•ç†: {checkpoint['processed_count']}")
    print(f"å¾…è™•ç†: {total_samples}")
    if workers > 1:
        print(f"ä¸¦ç™¼æ•¸é‡: {workers} workers")
    print("=" * 80 + "\n")

    start_time = time.time()
    last_checkpoint_time = start_time
    completed_count = 0

    # æº–å‚™ä»»å‹™æ•¸æ“š
    tasks = []
    for i, row in enumerate(calendar_df.itertuples(), 1):
        tasks.append({
            "index": i,
            "symbol": row.symbol,
            "year": row.year,
            "quarter": row.quarter,
            "record_id": row.id
        })

    # é¸æ“‡åŸ·è¡Œæ¨¡å¼ï¼šä¸²è¡Œæˆ–ä¸¦ç™¼
    if workers == 1:
        # ä¸²è¡Œæ¨¡å¼ï¼ˆåŸå§‹é‚è¼¯ï¼‰
        for task in tasks:
            i = task["index"]
            symbol = task["symbol"]
            year = task["year"]
            quarter = task["quarter"]
            record_id = task["record_id"]

            print(f"[{i}/{total_samples}] {symbol} {year}Q{quarter}...", end=" ", flush=True)

            result_data = process_single_case(task)

            if result_data["success"]:
                direction_score = result_data["direction_score"]
                tier = result_data["tier"]
                case_time = result_data["time_seconds"]
                print(f"âœ“ D{direction_score} {tier if tier else 'N/A'} ({case_time:.1f}s)")
            else:
                print(f"âœ— éŒ¯èª¤: {result_data['error']}")

            results.append(result_data)
            checkpoint["processed_ids"].append(record_id)
            checkpoint["processed_count"] += 1
            completed_count += 1

            # Checkpoint: æ¯ CHECKPOINT_INTERVAL ç­†æˆ–æœ€å¾Œä¸€ç­†
            if completed_count % CHECKPOINT_INTERVAL == 0 or completed_count == total_samples:
                elapsed = time.time() - last_checkpoint_time

                # è¨ˆç®—çµ±è¨ˆ
                stats = calculate_statistics(results)

                # å„²å­˜ checkpoint å’Œçµæœ
                save_checkpoint(checkpoint)
                save_results(results, checkpoint, stats)

                # æ‰“å°çµ±è¨ˆ
                print_statistics(stats, checkpoint["processed_count"])

                # ä¼°è¨ˆå‰©é¤˜æ™‚é–“
                if completed_count < total_samples:
                    avg_time_per_sample = elapsed / CHECKPOINT_INTERVAL
                    remaining_samples = total_samples - completed_count
                    estimated_remaining_time = avg_time_per_sample * remaining_samples

                    print(f"â±ï¸  é ä¼°å‰©é¤˜æ™‚é–“: {estimated_remaining_time/60:.1f} åˆ†é˜")
                    print()

                last_checkpoint_time = time.time()

    else:
        # ä¸¦ç™¼æ¨¡å¼
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_task = {executor.submit(process_single_case, task): task for task in tasks}

            # è™•ç†å®Œæˆçš„ä»»å‹™
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                i = task["index"]
                symbol = task["symbol"]
                year = task["year"]
                quarter = task["quarter"]
                record_id = task["record_id"]

                result_data = future.result()

                # è¼¸å‡ºçµæœ
                if result_data["success"]:
                    direction_score = result_data["direction_score"]
                    tier = result_data["tier"]
                    case_time = result_data["time_seconds"]
                    print(f"[{completed_count+1}/{total_samples}] {symbol} {year}Q{quarter} âœ“ D{direction_score} {tier if tier else 'N/A'} ({case_time:.1f}s)")
                else:
                    print(f"[{completed_count+1}/{total_samples}] {symbol} {year}Q{quarter} âœ— éŒ¯èª¤: {result_data['error']}")

                # ç·šç¨‹å®‰å…¨çš„æ›´æ–°
                with results_lock:
                    results.append(result_data)

                with checkpoint_lock:
                    checkpoint["processed_ids"].append(record_id)
                    checkpoint["processed_count"] += 1
                    completed_count += 1

                    # Checkpoint: æ¯ CHECKPOINT_INTERVAL ç­†æˆ–æœ€å¾Œä¸€ç­†
                    if completed_count % CHECKPOINT_INTERVAL == 0 or completed_count == total_samples:
                        elapsed = time.time() - last_checkpoint_time

                        # è¨ˆç®—çµ±è¨ˆ
                        stats = calculate_statistics(results)

                        # å„²å­˜ checkpoint å’Œçµæœ
                        save_checkpoint(checkpoint)
                        save_results(results, checkpoint, stats)

                        # æ‰“å°çµ±è¨ˆ
                        print_statistics(stats, checkpoint["processed_count"])

                        # ä¼°è¨ˆå‰©é¤˜æ™‚é–“
                        if completed_count < total_samples:
                            avg_time_per_sample = elapsed / CHECKPOINT_INTERVAL
                            remaining_samples = total_samples - completed_count
                            estimated_remaining_time = avg_time_per_sample * remaining_samples

                            print(f"â±ï¸  é ä¼°å‰©é¤˜æ™‚é–“: {estimated_remaining_time/60:.1f} åˆ†é˜")
                            print()

                        last_checkpoint_time = time.time()

    # æœ€çµ‚çµ±è¨ˆ
    total_time = time.time() - start_time
    final_stats = calculate_statistics(results)

    print("\n" + "=" * 80)
    print("ğŸ‰ å›æ¸¬å®Œæˆ")
    print("=" * 80)
    print(f"ç¸½æ™‚é–“: {total_time/3600:.2f} hours")
    print(f"ç¸½æ¨£æœ¬: {checkpoint['processed_count']}")
    print("=" * 80 + "\n")

    print_statistics(final_stats, checkpoint["processed_count"])


def main():
    parser = argparse.ArgumentParser(description="å¢é‡å›æ¸¬è…³æœ¬")
    parser.add_argument("--test", type=int, metavar="N",
                       help="æ¸¬è©¦æ¨¡å¼: åªè™•ç† N ç­†")
    parser.add_argument("--full", action="store_true",
                       help="å®Œæ•´å›æ¸¬: è™•ç†æ‰€æœ‰è¨˜éŒ„")
    parser.add_argument("--limit", type=int, metavar="N",
                       help="é™åˆ¶è™•ç†æ•¸é‡: æœ€å¤šè™•ç† N ç­† (å¯èˆ‡ --resume é…åˆ)")
    parser.add_argument("--workers", type=int, metavar="N", default=DEFAULT_WORKERS,
                       help=f"ä¸¦ç™¼æ•¸é‡ (é è¨­: {DEFAULT_WORKERS}, è¨­ç‚º 1 å‰‡ä¸²è¡Œ)")
    parser.add_argument("--resume", action="store_true",
                       help="å¾ checkpoint ç¹¼çºŒ")
    parser.add_argument("--reset", action="store_true",
                       help="é‡ç½® checkpoint å’Œçµæœ")

    args = parser.parse_args()

    # é‡ç½®
    if args.reset:
        if CHECKPOINT_FILE.exists():
            CHECKPOINT_FILE.unlink()
        if RESULTS_FILE.exists():
            RESULTS_FILE.unlink()
        print("âœ… Checkpoint å’Œçµæœå·²é‡ç½®")
        return

    # è¼‰å…¥è²¡å ±æ—¥æ›†
    print("ğŸ“… è¼‰å…¥è²¡å ±æ—¥æ›†...")
    calendar_df = load_earnings_calendar()

    if len(calendar_df) == 0:
        print("âŒ æœªæ‰¾åˆ°è²¡å ±è¨˜éŒ„")
        sys.exit(1)

    # åŸ·è¡Œå›æ¸¬
    if args.test:
        print(f"\nğŸ§ª æ¸¬è©¦æ¨¡å¼: è™•ç† {args.test} ç­†")
        run_backtest(calendar_df, max_samples=args.test, resume=False, workers=args.workers)
    elif args.limit:
        print(f"\nğŸ¯ é™åˆ¶æ¨¡å¼: è™•ç† {args.limit} ç­†")
        run_backtest(calendar_df, max_samples=args.limit, resume=args.resume, workers=args.workers)
    elif args.full:
        print("\nğŸš€ å®Œæ•´å›æ¸¬æ¨¡å¼")
        run_backtest(calendar_df, max_samples=None, resume=args.resume, workers=args.workers)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
